{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading needed modules. Please wait...\n",
      "Loading core modules...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Loading needed modules. Please wait...')\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "print('Loading core modules...')\n",
    "os.chdir('../Perceiver-Music-Transformer/')\n",
    "\n",
    "import TMIDIX\n",
    "\n",
    "from perceiver_ar_pytorch import PerceiverAR\n",
    "from autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "os.chdir('../')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AutoregressiveWrapper:\n\tsize mismatch for net.token_emb.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([387, 1024]).\n\tsize mismatch for net.to_logits.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([387, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     24\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(full_path_to_model_checkpoint)\n\u001b[0;32m---> 26\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(state_dict)\n\u001b[1;32m     28\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/music/lib/python3.9/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AutoregressiveWrapper:\n\tsize mismatch for net.token_emb.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([387, 1024]).\n\tsize mismatch for net.to_logits.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([387, 1024])."
     ]
    }
   ],
   "source": [
    "full_path_to_model_checkpoint = \"Perceiver-Solo-Piano-Model.pth\" #@param {type:\"string\"}\n",
    "\n",
    "print('Loading the model...')\n",
    "# Load model\n",
    "\n",
    "# constants\n",
    "\n",
    "SEQ_LEN = 4096 * 4 # Total of 16k\n",
    "PREFIX_SEQ_LEN = (4096 * 4) - 1024 # 15.3k\n",
    "\n",
    "model = PerceiverAR(\n",
    "    num_tokens = 512,\n",
    "    dim = 1024,\n",
    "    depth = 24,\n",
    "    heads = 16,\n",
    "    dim_head = 64,\n",
    "    cross_attn_dropout = 0.5,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    cross_attn_seq_len = PREFIX_SEQ_LEN\n",
    ")\n",
    "model = AutoregressiveWrapper(model)\n",
    "model.cuda()\n",
    "\n",
    "state_dict = torch.load(full_path_to_model_checkpoint)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "# Model stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import muspy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "itomori = muspy.read_musicxml(\"data/xml_files/itomori.xml\")\n",
    "itomori.write_midi(\"data/midi_files/itomori.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading custom MIDI file...\n",
      "Converting to MIDI. Please stand-by...\n",
      "Done! Enjoy! :)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#@title Load Seed/Custom MIDI\n",
    "full_path_to_custom_MIDI_file = \"data/midi_files/itomori.mid\" #@param {type:\"string\"}\n",
    "\n",
    "print('Loading custom MIDI file...')\n",
    "score = TMIDIX.midi2ms_score(open(full_path_to_custom_MIDI_file, 'rb').read())\n",
    "\n",
    "events_matrix = []\n",
    "\n",
    "itrack = 1\n",
    "\n",
    "#==================================================\n",
    "\n",
    "# Memories augmentator\n",
    "\n",
    "def augment(inputs):\n",
    "\n",
    "  outs = []\n",
    "  outy = []\n",
    "\n",
    "  for i in range(1, 12):\n",
    "\n",
    "    out1 = []\n",
    "    out2 = []\n",
    "\n",
    "    for j in range(0, len(inputs), 4):\n",
    "      note = inputs[j:j+4]\n",
    "      aug_note1 = copy.deepcopy(note)\n",
    "      aug_note2 = copy.deepcopy(note)\n",
    "      aug_note1[2] += i\n",
    "      aug_note2[2] -= i\n",
    "\n",
    "      out1.append(aug_note1)\n",
    "      out2.append(aug_note2)\n",
    "\n",
    "    outs.append(out1[random.randint(0, int(len(out1) / 2)):random.randint(int(len(out1) / 2), len(out1))])\n",
    "    outs.append(out2[random.randint(0, int(len(out2) / 2)):random.randint(int(len(out2) / 2), len(out2))])\n",
    "\n",
    "  for i in range(64):\n",
    "    outy.extend(random.choice(outs))\n",
    "\n",
    "  outy1 = []\n",
    "  for o in outy:\n",
    "    outy1.extend(o)\n",
    "\n",
    "  return outy1\n",
    "\n",
    "#==================================================\n",
    "\n",
    "while itrack < len(score):\n",
    "    for event in score[itrack]:         \n",
    "        if event[0] == 'note' and event[3] != 9:\n",
    "            events_matrix.append(event)\n",
    "    itrack += 1\n",
    "\n",
    "if len(events_matrix) > 0:\n",
    "\n",
    "    # Sorting...\n",
    "    events_matrix.sort(key=lambda x: x[4], reverse=True)\n",
    "    events_matrix.sort(key=lambda x: x[1])\n",
    "\n",
    "    # recalculating timings\n",
    "    for e in events_matrix:\n",
    "        e[1] = int(e[1] / 10)\n",
    "        e[2] = int(e[2] / 20)\n",
    "\n",
    "    # final processing...\n",
    "    inputs = []\n",
    "    \n",
    "    inputs.extend([126+0, 126+128, 0+256, 0+384]) # Intro/Zero sequence\n",
    "\n",
    "    pe = events_matrix[0]\n",
    "    for e in events_matrix:\n",
    "\n",
    "        time = max(0, min(126, e[1]-pe[1]))\n",
    "        dur = max(1, min(126, e[2]))\n",
    "\n",
    "        ptc = max(1, min(126, e[4]))\n",
    "        vel = max(1, min(126, e[5]))\n",
    "\n",
    "        inputs.extend([time+0, dur+128, ptc+256, vel+384])\n",
    "\n",
    "        pe = e\n",
    "\n",
    "# =================================\n",
    "\n",
    "out1 = inputs\n",
    "\n",
    "if len(out1) != 0:\n",
    "    \n",
    "    song = out1\n",
    "    song_f = []\n",
    "    time = 0\n",
    "    dur = 0\n",
    "    vel = 0\n",
    "    pitch = 0\n",
    "    channel = 0\n",
    "\n",
    "    son = []\n",
    "\n",
    "    song1 = []\n",
    "\n",
    "    for s in song:\n",
    "      if s > 127:\n",
    "        son.append(s)\n",
    "\n",
    "      else:\n",
    "        if len(son) == 4:\n",
    "          song1.append(son)\n",
    "        son = []\n",
    "        son.append(s)\n",
    "    \n",
    "    for s in song1:\n",
    "\n",
    "        channel = 0 # Piano\n",
    "\n",
    "        time += s[0] * 10\n",
    "            \n",
    "        dur = (s[1]-128) * 20\n",
    "        \n",
    "        pitch = (s[2]-256)\n",
    "\n",
    "        vel = (s[3]-384)\n",
    "\n",
    "        if pitch != 0:\n",
    "                                  \n",
    "          song_f.append(['note', time, dur, channel, pitch, vel ])\n",
    "\n",
    "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                        output_signature = 'Perceiver',  \n",
    "                                                        output_file_name = 'Perceiver-Music-Composition', \n",
    "                                                        track_name='Project Los Angeles',\n",
    "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],\n",
    "                                                        number_of_ticks_per_quarter=500)\n",
    "\n",
    "    print('Done!')\n",
    "\n",
    "# print('Displaying resulting composition...')\n",
    "# fname = 'Perceiver-Music-Composition'\n",
    "\n",
    "# x = []\n",
    "# y =[]\n",
    "# c = []\n",
    "\n",
    "# colors = ['red', 'yellow', 'green', 'cyan', 'blue', 'pink', 'orange', 'purple', 'gray', 'white', 'gold', 'silver']\n",
    "\n",
    "# for s in song_f:\n",
    "#   x.append(s[1] / 1000)\n",
    "#   y.append(s[4])\n",
    "#   c.append(colors[s[3]])\n",
    "\n",
    "# FluidSynth().midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
    "# display(Audio(str(fname + '.wav'), rate=16000))\n",
    "\n",
    "# plt.figure(figsize=(14,5))\n",
    "# ax=plt.axes(title=fname)\n",
    "# ax.set_facecolor('black')\n",
    "\n",
    "# plt.scatter(x,y, c=c)\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.ylabel(\"Pitch\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = [126+0, 126+128, 0+256, 0+384] * ((PREFIX_SEQ_LEN) // 4) + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15360"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREFIX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16576"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = inputs * math.ceil((4096 * 4) / len(inputs))\n",
    "# inp = inp[:(4096 * 4)]\n",
    "\n",
    "# inp = inp[(512+len(inputs[:256])):] + inputs[:256]\n",
    "\n",
    "inp = torch.LongTensor(inp[:SEQ_LEN]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = copy.deepcopy(inputs[:64])\n",
    "inp = inputs * math.ceil((4096 * 4) / len(inputs))\n",
    "\n",
    "inp = inp[:(4096 * 4)]\n",
    "\n",
    "inp = inp[(512+len(out2)):] + out2\n",
    "\n",
    "inp1 = torch.LongTensor(inp).cuda()\n",
    "out = inp1[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mnet(\n\u001b[1;32m      2\u001b[0m                 out[:, \u001b[39m-\u001b[39;49mmodel\u001b[39m.\u001b[39;49mmax_seq_len:],\n\u001b[1;32m      3\u001b[0m             )[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/music/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data1/zachary/music-difficulty/Perceiver-Music-Transformer/perceiver_ar_pytorch.py:308\u001b[0m, in \u001b[0;36mPerceiverAR.forward\u001b[0;34m(self, x, prefix_mask, labels)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39m# initial perceiver attention and feedforward (one cross attention)\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39mfor\u001b[39;00m cross_attn, ff \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperceive_layers:\n\u001b[0;32m--> 308\u001b[0m     x \u001b[39m=\u001b[39m cross_attn(x, prefix, context_mask \u001b[39m=\u001b[39;49m prefix_mask, rotary_pos_emb \u001b[39m=\u001b[39;49m rotary_pos_emb) \u001b[39m+\u001b[39m x\n\u001b[1;32m    309\u001b[0m     x \u001b[39m=\u001b[39m ff(x) \u001b[39m+\u001b[39m x\n\u001b[1;32m    311\u001b[0m \u001b[39m# layers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/music/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data1/zachary/music-difficulty/Perceiver-Music-Transformer/perceiver_ar_pytorch.py:162\u001b[0m, in \u001b[0;36mCausalPrefixAttention.forward\u001b[0;34m(self, x, context, context_mask, rotary_pos_emb)\u001b[0m\n\u001b[1;32m    158\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_norm(context)\n\u001b[1;32m    160\u001b[0m \u001b[39m# derive queries, keys, values\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_q(x)\n\u001b[1;32m    164\u001b[0m k_input, v_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_kv(x)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    165\u001b[0m k_context, v_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_kv(context)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/music/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/music/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "logits = model.net(\n",
    "                out[:, -model.max_seq_len:],\n",
    "            )[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9af94f28876c9d3667e32c3f57cf369bad4902fd5c407457529a66e9442bcdc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
